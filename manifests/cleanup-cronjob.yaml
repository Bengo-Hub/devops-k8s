---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: pod-cleanup
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pod-cleanup
rules:
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list", "delete"]
  - apiGroups: [""]
    resources: ["namespaces"]
    verbs: ["get", "list"]
  - apiGroups: ["apps"]
    resources: ["deployments", "replicasets"]
    verbs: ["get", "list", "patch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: pod-cleanup
subjects:
  - kind: ServiceAccount
    name: pod-cleanup
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: pod-cleanup
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cleanup-failed-pods
  namespace: kube-system
spec:
  # Run every 30 minutes
  schedule: "*/30 * * * *"
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: pod-cleanup
        spec:
          serviceAccountName: pod-cleanup
          restartPolicy: OnFailure
          containers:
            - name: cleanup
              image: bitnami/kubectl:latest
              imagePullPolicy: IfNotPresent
              command:
                - /bin/bash
                - -c
                - |
                  #!/bin/bash
                  set -euo pipefail
                  
                  echo "üßπ Starting automated pod cleanup at $(date)"
                  
                  # Define namespaces to scan (exclude kube-system, kube-public, kube-node-lease)
                  EXCLUDE_NS="kube-system|kube-public|kube-node-lease|calico-system|calico-apiserver"
                  NAMESPACES=$(kubectl get namespaces -o jsonpath='{.items[*].metadata.name}' | tr ' ' '\n' | grep -vE "^($EXCLUDE_NS)$" || true)
                  
                  TOTAL_DELETED=0
                  
                  # Function to delete pods by status phase
                  cleanup_by_phase() {
                    local PHASE=$1
                    echo "üîç Scanning for pods in $PHASE state..."
                    
                    for NS in $NAMESPACES; do
                      PODS=$(kubectl get pods -n "$NS" --field-selector=status.phase="$PHASE" -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo "")
                      
                      if [ -n "$PODS" ]; then
                        echo "  üì¶ Deleting $PHASE pods in namespace $NS: $PODS"
                        kubectl delete pods -n "$NS" --field-selector=status.phase="$PHASE" --grace-period=0 --force 2>/dev/null || true
                        COUNT=$(echo "$PODS" | wc -w)
                        TOTAL_DELETED=$((TOTAL_DELETED + COUNT))
                      fi
                    done
                  }
                  
                  # Function to prevent recreation of failed pods by scaling down failed deployments
                  disable_failed_deployments() {
                    echo "üîç Scanning for deployments with repeated pod failures..."
                    
                    for NS in $NAMESPACES; do
                      # Find deployments where all pods are failing (0 ready)
                      FAILED_DEPLOYS=$(kubectl get deployments -n "$NS" -o json 2>/dev/null | \
                        jq -r '.items[] | select(.status.replicas > 0 and .status.readyReplicas == 0 and .status.replicas == .status.unavailableReplicas) | .metadata.name' || echo "")
                      
                      if [ -n "$FAILED_DEPLOYS" ]; then
                        for DEPLOY in $FAILED_DEPLOYS; then
                          # Check if deployment has been failing for >10 minutes
                          AGE_SECONDS=$(kubectl get deployment -n "$NS" "$DEPLOY" -o jsonpath='{.metadata.creationTimestamp}' | xargs -I {} date -d {} +%s 2>/dev/null || echo "0")
                          NOW_SECONDS=$(date +%s)
                          AGE=$((NOW_SECONDS - AGE_SECONDS))
                          
                          if [ $AGE -gt 600 ]; then
                            echo "  ‚è∏Ô∏è  Scaling down failed deployment $NS/$DEPLOY (failing for ${AGE}s)"
                            kubectl scale deployment -n "$NS" "$DEPLOY" --replicas=0 2>/dev/null || true
                            kubectl annotate deployment -n "$NS" "$DEPLOY" \
                              "bengobox.dev/auto-disabled=true" \
                              "bengobox.dev/disabled-reason=repeated-pod-failures" \
                              "bengobox.dev/disabled-at=$(date -Iseconds)" \
                              --overwrite 2>/dev/null || true
                          fi
                        done
                      fi
                    done
                  }
                  
                  # Cleanup Failed pods
                  cleanup_by_phase "Failed"
                  
                  # Cleanup Pending pods (older than 5 minutes)
                  echo "üîç Scanning for stale Pending pods..."
                  for NS in $NAMESPACES; do
                    STALE_PENDING=$(kubectl get pods -n "$NS" --field-selector=status.phase=Pending -o json 2>/dev/null | \
                      jq -r --arg now "$(date +%s)" '.items[] | select((($now | tonumber) - (.metadata.creationTimestamp | fromdateiso8601)) > 300) | .metadata.name' || echo "")
                    
                    if [ -n "$STALE_PENDING" ]; then
                      echo "  üì¶ Deleting stale Pending pods in $NS: $STALE_PENDING"
                      for POD in $STALE_PENDING; do
                        kubectl delete pod -n "$NS" "$POD" --grace-period=0 --force 2>/dev/null || true
                        TOTAL_DELETED=$((TOTAL_DELETED + 1))
                      done
                    fi
                  done
                  
                  # Disable deployments with repeated failures
                  disable_failed_deployments
                  
                  # Cleanup completed Jobs older than 1 hour
                  echo "üîç Cleaning up old completed jobs..."
                  for NS in $NAMESPACES; do
                    OLD_JOBS=$(kubectl get jobs -n "$NS" -o json 2>/dev/null | \
                      jq -r --arg now "$(date +%s)" '.items[] | select(.status.succeeded == 1 and (($now | tonumber) - (.status.completionTime | fromdateiso8601)) > 3600) | .metadata.name' || echo "")
                    
                    if [ -n "$OLD_JOBS" ]; then
                      for JOB in $OLD_JOBS; do
                        kubectl delete job -n "$NS" "$JOB" --cascade=foreground 2>/dev/null || true
                      done
                    fi
                  done
                  
                  echo "‚úÖ Cleanup completed at $(date)"
                  echo "   Total pods deleted: $TOTAL_DELETED"
                  
                  # Check current pod count
                  RUNNING_PODS=$(kubectl get pods --all-namespaces --field-selector=status.phase=Running --no-headers 2>/dev/null | wc -l)
                  echo "   Current running pods: $RUNNING_PODS/110"
                  
                  if [ $RUNNING_PODS -ge 100 ]; then
                    echo "‚ö†Ô∏è  WARNING: Cluster approaching pod limit ($RUNNING_PODS/110)"
                  fi
              resources:
                requests:
                  cpu: 10m
                  memory: 32Mi
                limits:
                  cpu: 100m
                  memory: 128Mi

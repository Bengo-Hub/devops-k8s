image:
  repository: docker.io/codevertex/erp-api
  tag: "95841b51"
  pullSecrets:
    - name: registry-credentials
# Pod-level security context - CRITICAL for PVC permissions
# fsGroup ensures mounted volumes are writable by app user (UID 1000)
securityContext:
  runAsUser: 1000
  runAsGroup: 1000
  fsGroup: 1000
  runAsNonRoot: true
# Deployment strategy for zero-downtime updates
replicaCount: 2
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 1
    maxUnavailable: 0
service:
  port: 80
  targetPort: 4000
minReadySeconds: 60
# Health check configuration for Django API
# Adjusted for migration execution time (can take 5-10 minutes on first deploy)
# INCREASED STARTUP TIME: Migrations can take 5-10 minutes even with better resources
# REDUCED READINESS: If app is ready, health checks should be quick (< 1 minute)
healthCheck:
  enabled: true
  startup:
    httpGet:
      path: /api/v1/core/health/
      port: http
    # Allow time for migrations + cold starts
    # With idle db: ~30s, with migrations: 5-10min (first pod only)
    initialDelaySeconds: 60 # Reduced: migrations run in init container, not in app
    periodSeconds: 20
    timeoutSeconds: 10
    failureThreshold: 25 # Allow ~500s total (60 + 20*25) for slow migrations
  readiness:
    httpGet:
      path: /api/v1/core/health/
      port: http
    # Once migrations are done, readiness should be fast
    initialDelaySeconds: 120 # Give app time to initialize after migrations complete
    periodSeconds: 15 # More frequent checks for faster responsiveness
    timeoutSeconds: 10
    failureThreshold: 4 # Fail after ~180s (120 + 15*4) if still not ready
  liveness:
    httpGet:
      path: /api/v1/core/health/
      port: http
    initialDelaySeconds: 180 # App should be stable by now
    periodSeconds: 30
    timeoutSeconds: 15
    failureThreshold: 5
# Production resource limits for Django API with large ERP schema
# This app requires high memory due to:
# - 30+ Django apps with complex models
# - Heavy migrations (300+ tables)
# - Real-time WebSocket connections
# - Async task processing
# INCREASED RESOURCES: 20-30min rollout indicates insufficient CPU for migrations
resources:
  requests:
    cpu: 1500m # INCREASED: From 500m - heavy migrations need more CPU
    memory: 4Gi # INCREASED: From 3Gi - better for concurrent migrations + app startup
  limits:
    cpu: 4000m # INCREASED: From 3000m - allow more CPU during peak migration load
    memory: 8Gi # INCREASED: From 6Gi - prevent OOM during heavy migrations
# Horizontal autoscaling based on CPU and memory
autoscaling:
  enabled: true
  minReplicas: 2 # Minimum 2 for high availability
  maxReplicas: 6 # INCREASED: From 4 - allow more scaling with higher resource baseline
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 75
  scaleDown:
    stabilizationWindowSeconds: 300
    policies:
      - type: Percent
        value: 50
        periodSeconds: 60
  scaleUp:
    stabilizationWindowSeconds: 60
    policies:
      - type: Percent
        value: 100
        periodSeconds: 30
ingress:
  enabled: true
  ingressClassName: nginx
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    # WebSocket support for Django Channels (payroll real-time updates)
    nginx.ingress.kubernetes.io/websocket-services: erp-api-app
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
    # Serve media files directly from Django (for production media serving)
    nginx.ingress.kubernetes.io/proxy-body-size: "50m"
    nginx.ingress.kubernetes.io/configuration-snippet: |
      location /media/ {
        proxy_pass http://erp-api-app.erp.svc.cluster.local:80;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
      }
      location /static/ {
        proxy_pass http://erp-api-app.erp.svc.cluster.local:80;
        proxy_set_header Host $host;
      }
  hosts:
    - host: erpapi.masterspace.co.ke
      paths:
        - path: /
          pathType: Prefix
  tls:
    - hosts:
        - erpapi.masterspace.co.ke
      secretName: erpapi-masterspace-tls
envFromSecret: erp-api-env
# ============================================================================
# MIGRATIONS AND SEEDING
# ============================================================================
# ERP-API Django migrations strategy:
# 1. Migrations run in init container BEFORE main app starts
# 2. Uses idempotent `migrate --noinput` (safe to run multiple times)
# 3. Each pod waits for migrations to complete before starting app
# 4. Heavy migrations (300+ tables) require 3-6Gi memory
# 5. Fixed: Migration failures were causing pods to fail health checks
migrations:
  enabled: true # Enable init container for reliable migrations
  runOnStartup: true # Wait for migrations to complete before app starts
  resources:
    requests:
      cpu: 1000m # INCREASED: From 500m - migrations are very CPU-intensive
      memory: 4Gi # INCREASED: From 3Gi - better for large schema migrations
    limits:
      cpu: 3000m # INCREASED: From 2000m - allow more CPU for parallel migration operations
      memory: 8Gi # INCREASED: From 6Gi - prevent OOM during complex migrations
# Seeding configuration
seed:
  enabled: true # Enable seed data creation via Django management command
  command: "python manage.py seed_data" # Custom management command for initial data
  runAfterMigrations: true # Run seeds only after migrations complete
  resources:
    requests:
      cpu: 200m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 1Gi
# ServiceMonitor configuration (requires Prometheus Operator)
serviceMonitor:
  enabled: false # Disabled - Prometheus Operator CRDs not installed
  namespace: infra
  interval: 30s
  scrapeTimeout: 10s
  labels:
    release: prometheus
# Persistent storage for media uploads
persistence:
  enabled: true
  size: 20Gi
  mountPath: /app/media
  accessMode: ReadWriteOnce
  storageClass: local-path
# Enhanced HPA with custom metrics
autoscalingCustomMetrics:
  enabled: false # Disabled - using basic HPA for now
  # Custom metrics for intelligent scaling (requires metrics adapter)
  customMetrics:
    - type: Pods
      metricName: bengoerp_api_http_requests_per_second
      targetType: AverageValue
      targetAverageValue: 50
    - type: Pods
      metricName: bengoerp_api_db_query_duration_seconds
      targetType: AverageValue
      targetAverageValue: 1.0
    - type: Pods
      metricName: bengoerp_api_active_users
      targetType: AverageValue
      targetAverageValue: 100
# Production-ready VPA configuration (DISABLED to prevent pod evictions during migrations)
verticalPodAutoscaling:
  enabled: false # DISABLED - VPA was causing pod evictions during migrations
  updateMode: "Off" # Off to prevent disruption
  minCPU: 500m
  maxCPU: 3000m
  minMemory: 3Gi
  maxMemory: 6Gi
  controlledResources: ["cpu", "memory"]
  controlledValues: RequestsAndLimits
  recommendationMode: false # Apply recommendations automatically in production
# Celery Worker configuration for async task processing
celeryWorker:
  enabled: true
  replicaCount: 1
  minReadySeconds: 30
  app: ProcureProKEAPI
  logLevel: info
  concurrency: 4 # Optimized for 12-core server
  maxTasksPerChild: 100
  resources:
    requests:
      cpu: 500m # INCREASED: From 300m - better task processing performance
      memory: 1.5Gi # INCREASED: From 1Gi - workers load Django models
    limits:
      cpu: 2000m # INCREASED: From 1500m - allow more CPU for heavy tasks
      memory: 3Gi # INCREASED: From 2Gi - prevent OOM during heavy processing
  livenessProbe:
    enabled: true
    initialDelaySeconds: 90 # Allow time for worker to start and connect to broker
    periodSeconds: 60
    timeoutSeconds: 30 # INCREASED: celery inspect ping can be slow
    failureThreshold: 5 # More tolerance for transient failures
  readinessProbe:
    enabled: true
    initialDelaySeconds: 60 # Worker should be ready faster than liveness
    periodSeconds: 30
    timeoutSeconds: 30 # INCREASED: celery inspect ping can be slow
    failureThreshold: 5 # More tolerance for transient failures
# Celery Beat scheduler for periodic tasks
celeryBeat:
  enabled: true
  minReadySeconds: 30
  app: ProcureProKEAPI
  logLevel: info
  scheduler: django_celery_beat.schedulers:DatabaseScheduler
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi
  persistence:
    enabled: false

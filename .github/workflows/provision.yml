name: Provision Cluster Infrastructure

on:
  push:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      enable_cleanup:
        description: 'Enable cluster cleanup (DANGEROUS - deletes all data). Default: true (enabled). Set to false to disable cleanup.'
        required: false
        default: false
        type: boolean
      force_reprovision:
        description: 'Force reprovisioning (cleanup + reinstall). Requires enable_cleanup=true.'
        required: false
        default: false
        type: boolean

jobs:
  provision:
    runs-on: ubuntu-latest
    permissions:
      contents: read
    env:
      # Configurable domains and IPs via GitHub secrets (with defaults)
      # Priority: GitHub secrets first, fallback to defaults
      ARGOCD_DOMAIN: ${{ secrets.ARGOCD_DOMAIN || 'argocd.masterspace.co.ke' }}
      GRAFANA_DOMAIN: ${{ secrets.GRAFANA_DOMAIN || 'grafana.masterspace.co.ke' }}
      DB_NAMESPACE: ${{ secrets.DB_NAMESPACE || 'infra' }}
      MONITORING_NAMESPACE: ${{ secrets.MONITORING_NAMESPACE || 'infra' }}
      CLUSTER_NAME: ${{ secrets.CLUSTER_NAME || 'mss-prod' }}
      # Cleanup DISABLED by default to prevent accidental data loss
      # Can be enabled via workflow_dispatch input or GitHub secret ENABLE_CLEANUP=true
      # Priority: workflow input > GitHub secret > default FALSE
      ENABLE_CLEANUP: ${{ github.event_name == 'workflow_dispatch'
        && (github.event.inputs.enable_cleanup == 'true' && 'true' || 'false')
        || (secrets.ENABLE_CLEANUP == 'true' && 'true' || 'false') }}
      FORCE_REPROVISION: ${{ github.event_name == 'workflow_dispatch'
        && (github.event.inputs.force_reprovision == 'true' && 'true' || 'false')
        || (secrets.FORCE_REPROVISION == 'true' && 'true' || 'false') }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install DevOps tools (kubectl, helm, yq, argocd)
        uses: ./.github/actions/install-devops-tools
        with:
          install_trivy: 'false'

      - name: Install PostgreSQL client and Python (psql, python3) [idempotent]
        run: |
          echo "::group::Installing PostgreSQL client and Python"
          # Check if packages are already installed
          MISSING_PKGS=()
          
          # Check for PostgreSQL client (psql command)
          if ! command -v psql &> /dev/null; then
            MISSING_PKGS+=("postgresql-client" "postgresql-client-common")
          else
            echo "‚úì PostgreSQL client (psql) already installed"
            psql --version
          fi
          
          # Check for Python3 (required for Superset secret generation)
          if ! command -v python3 &> /dev/null; then
            MISSING_PKGS+=("python3")
          else
            echo "‚úì Python3 already installed"
            python3 --version
          fi
          
          # Note: jq and curl are already installed by install-devops-tools action
          # Only install if missing (defensive check)
          if ! command -v jq &> /dev/null; then
            MISSING_PKGS+=("jq")
          fi
          
          if ! command -v curl &> /dev/null; then
            MISSING_PKGS+=("curl")
          fi
          
          # Only run apt-get if packages are missing
          if [ ${#MISSING_PKGS[@]} -eq 0 ]; then
            echo "‚úì All required packages already installed"
            echo "::endgroup::"
            exit 0
          fi
          
          echo "Installing missing packages: ${MISSING_PKGS[*]}"
          sudo apt-get update -y
          sudo apt-get install -y "${MISSING_PKGS[@]}"
          
          # Verify installation
          if command -v psql &> /dev/null; then
            echo "‚úì PostgreSQL client installed successfully"
            psql --version
          fi
          echo "::endgroup::"

      - name: Get VPS IP and ensure running (via Contabo API)
        id: contabo_vps
        env:
          CONTABO_CLIENT_ID: ${{ secrets.CONTABO_CLIENT_ID }}
          CONTABO_CLIENT_SECRET: ${{ secrets.CONTABO_CLIENT_SECRET }}
          CONTABO_API_USERNAME: ${{ secrets.CONTABO_API_USERNAME }}
          CONTABO_API_PASSWORD: ${{ secrets.CONTABO_API_PASSWORD }}
          CONTABO_INSTANCE_ID: ${{ secrets.CONTABO_INSTANCE_ID || '14285715' }}
          SSH_HOST: ${{ secrets.SSH_HOST || '' }}
        run: |
          echo "::group::Contabo API - VPS Management"
          
          # Priority: SSH_HOST secret > Contabo API
          if [ -n "${SSH_HOST}" ] && [ "${SSH_HOST}" != "YOUR_VPS_IP" ]; then
            echo "‚úÖ Using VPS IP from SSH_HOST secret: ${SSH_HOST}"
            echo "vps_ip=${SSH_HOST}" >> $GITHUB_OUTPUT
            echo "vps_source=github_secret" >> $GITHUB_OUTPUT
            echo "::endgroup::"
            exit 0
          fi
            
            # Check if Contabo API credentials are available
            if [ -z "${CONTABO_CLIENT_ID}" ] || [ -z "${CONTABO_CLIENT_SECRET}" ] || \
               [ -z "${CONTABO_API_USERNAME}" ] || [ -z "${CONTABO_API_PASSWORD}" ]; then
            echo "‚ö†Ô∏è  Contabo API credentials not available"
            echo "‚ö†Ô∏è  Please set SSH_HOST secret with VPS IP address"
              echo "‚ö†Ô∏è  Or configure Contabo API credentials (CONTABO_CLIENT_ID, CONTABO_CLIENT_SECRET, etc.)"
            echo "vps_ip=" >> $GITHUB_OUTPUT
            echo "vps_source=none" >> $GITHUB_OUTPUT
              echo "::endgroup::"
              exit 0
            fi
            
          # Source Contabo API helper
            source ./scripts/tools/contabo-api.sh
            
          # Get access token
          ACCESS_TOKEN=$(get_contabo_token)
          if [ $? -ne 0 ]; then
            echo "‚ùå Failed to get Contabo API token"
            echo "vps_ip=" >> $GITHUB_OUTPUT
            echo "vps_source=none" >> $GITHUB_OUTPUT
            echo "::endgroup::"
            exit 0
          fi
          
          # Ensure VPS is running
          echo "Ensuring VPS is running..."
          ensure_vps_running "$ACCESS_TOKEN"
          
          # Get VPS IP
          VPS_IP=$(get_vps_ip_from_contabo "$ACCESS_TOKEN")
          if [ $? -eq 0 ] && [ -n "$VPS_IP" ]; then
            echo "‚úÖ Got VPS IP from Contabo API: ${VPS_IP}"
            echo "vps_ip=${VPS_IP}" >> $GITHUB_OUTPUT
            echo "vps_source=contabo_api" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Failed to get VPS IP from Contabo API"
            echo "vps_ip=" >> $GITHUB_OUTPUT
            echo "vps_source=none" >> $GITHUB_OUTPUT
          fi
          
          echo "::endgroup::"

      - name: Configure kubeconfig
        id: kubeconfig_setup
        env:
          KUBE_CONFIG_B64: ${{ secrets.KUBE_CONFIG }}
        run: |
          echo "::group::Checking KUBE_CONFIG"
          if [ -z "$KUBE_CONFIG_B64" ]; then
            echo "‚ùå KUBE_CONFIG secret not provided."
            echo ""
            echo "üìã MANUAL SETUP REQUIRED:"
            echo "This workflow requires a Kubernetes cluster to be set up manually first."
            echo "Please follow the manual setup guide: docs/contabo-setup-kubeadm.md"
            echo ""
            echo "After completing manual setup:"
            echo "1. Get your kubeconfig from the VPS: cat ~/.kube/config | base64 -w 0"
            echo "2. Add it as GitHub secret: KUBE_CONFIG"
            echo "3. Re-run this workflow to provision infrastructure"
            echo ""
            echo "skip_provision=true" >> $GITHUB_ENV
            echo "kubeconfig_ready=false" >> $GITHUB_OUTPUT
            echo "::endgroup::"
            exit 0
          fi
          echo "‚úÖ KUBE_CONFIG secret found"
          echo "::endgroup::"
          
          echo "::group::Decoding and configuring kubeconfig"
          mkdir -p ~/.kube
          
          # Clean the base64 string (remove whitespace, newlines, etc.)
          CLEAN_B64=$(echo "$KUBE_CONFIG_B64" | tr -d '[:space:]')
          
          # Validate base64 format
          if ! echo "$CLEAN_B64" | base64 -d >/dev/null 2>&1; then
            echo "‚ùå Invalid base64 format in KUBE_CONFIG secret"
            echo ""
            echo "Troubleshooting:"
            echo "1. Make sure you copied the ENTIRE base64 output from setup-cluster.sh"
            echo "2. The kubeconfig should be base64-encoded WITHOUT line breaks"
            echo "3. To regenerate on VPS: cat /etc/kubernetes/admin.conf | base64 -w 0"
            echo "4. Or: cat ~/.kube/config | base64 -w 0"
            echo ""
            echo "Current secret length: ${#CLEAN_B64} characters"
            echo "First 50 chars: ${CLEAN_B64:0:50}..."
            exit 1
          fi
          
          # Decode and write kubeconfig
          echo "$CLEAN_B64" | base64 -d > ~/.kube/config
          
          # Verify kubeconfig file was created and has content
          if [ ! -s ~/.kube/config ]; then
            echo "‚ùå Kubeconfig file is empty after decoding"
            exit 1
          fi
          
          chmod 600 ~/.kube/config
          echo "‚úÖ Kubeconfig written to ~/.kube/config ($(wc -c < ~/.kube/config) bytes)"
          echo "::endgroup::"
          
          echo "::group::Testing cluster connectivity"
          if kubectl cluster-info >/dev/null 2>&1; then
            echo "‚úÖ Successfully connected to Kubernetes cluster"
            kubectl get nodes || true
            
            # Check if cluster is ready for provisioning
            READY_NODES=$(kubectl get nodes --no-headers 2>/dev/null | grep -c "Ready" || echo "0")
            if [ "$READY_NODES" -gt 0 ]; then
              echo "‚úÖ Cluster is ready (${READY_NODES} node(s) Ready)"
              echo "skip_provision=false" >> $GITHUB_ENV
              echo "kubeconfig_ready=true" >> $GITHUB_OUTPUT
            else
              echo "‚ö†Ô∏è  No Ready nodes found. Cluster may not be fully initialized."
            echo "skip_provision=false" >> $GITHUB_ENV
              echo "kubeconfig_ready=true" >> $GITHUB_OUTPUT
            fi
          else
            echo "‚ùå Failed to connect to cluster. Check your KUBE_CONFIG."
            echo "skip_provision=true" >> $GITHUB_ENV
            echo "kubeconfig_ready=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          echo "::endgroup::"

      - name: Ensure scripts executable
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        run: |
          echo "::group::Making scripts executable"
          find scripts -type f -name "*.sh" -exec chmod +x {} \;
          echo "‚úÖ All scripts are now executable"
          echo "::endgroup::"

      - name: Cleanup cluster (opt-in only)
        if: ${{ steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true' && env.ENABLE_CLEANUP == 'true' }}
        env:
          ENABLE_CLEANUP: ${{ env.ENABLE_CLEANUP }}
          FORCE_CLEANUP: 'true'
        run: |
          echo "::group::Cluster Cleanup & Reprovisioning"
          echo "ENABLE_CLEANUP=${ENABLE_CLEANUP}"
          echo "FORCE_CLEANUP=${FORCE_CLEANUP}"
          echo "üßπ Cleanup is ENABLED - This will delete ALL applications and data!"
          echo "‚ö†Ô∏è  Deleting namespaces: erp, truload, infra, argocd, monitoring, cafe, treasury, notifications, auth-service"
          echo "‚ö†Ô∏è  System namespaces preserved: kube-system, calico-system, cert-manager, ingress-nginx"
          export ENABLE_CLEANUP=true
          export FORCE_CLEANUP=true
          ./scripts/cluster/cleanup-cluster.sh
          echo "‚úÖ Cleanup complete"
          echo "::endgroup::"

      - name: Create required namespaces
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        run: |
          echo "::group::Creating required namespaces"
          
          # List of namespaces required by the cluster
          # Create these FIRST, independent of registry credentials
          NAMESPACES="infra argocd erp truload cafe treasury notifications auth inventory logistics pos iot isp-billing ordering projects subscription ticketing"
          
          for ns in $NAMESPACES; do
            if ! kubectl get namespace "$ns" >/dev/null 2>&1; then
              kubectl create namespace "$ns" >/dev/null 2>&1 || true
              echo "‚úì Created namespace: $ns"
            else
              echo "‚úì Namespace already exists: $ns"
            fi
          done
          
          echo "‚úÖ All required namespaces ensured"
          echo "::endgroup::"

      - name: Create registry-credentials secrets in all namespaces
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        env:
          REGISTRY_USERNAME: ${{ secrets.REGISTRY_USERNAME || 'codevertex' }}
          REGISTRY_PASSWORD: ${{ secrets.REGISTRY_PASSWORD }}
          REGISTRY_EMAIL: ${{ secrets.REGISTRY_EMAIL || 'codevertexitsolutions@gmail.com' }}
          REGISTRY_SERVER: docker.io
        run: |
          echo "::group::Creating registry-credentials secrets"
          
          if [ -z "${REGISTRY_PASSWORD}" ]; then
            echo "‚ö†Ô∏è  REGISTRY_PASSWORD not set - skipping registry secret creation"
            echo "‚ö†Ô∏è  Pods may fail with ImagePullBackOff if Docker Hub rate limit is exceeded"
            echo "‚ö†Ô∏è  Set REGISTRY_PASSWORD GitHub secret to enable Docker Hub authentication"
            echo "‚ö†Ô∏è  Note: Namespaces were already created in previous step"
            echo "::endgroup::"
            exit 0
          fi
          
          echo "Using registry credentials:"
          echo "  Username: ${REGISTRY_USERNAME}"
          echo "  Email: ${REGISTRY_EMAIL}"
          echo "  Server: ${REGISTRY_SERVER}"
          
          patch_default_sa() {
            local ns="$1"
            # Ensure default service account exists (it should)
            if ! kubectl get sa default -n "$ns" >/dev/null 2>&1; then
              return 0
            fi
            # Check if imagePullSecrets already includes registry-credentials
            IPS_NAMES=$(kubectl get sa default -n "$ns" -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "")
            if echo "$IPS_NAMES" | grep -qw "registry-credentials"; then
              echo "  - default ServiceAccount already references imagePullSecrets: registry-credentials"
            else
              # If imagePullSecrets array exists, append; else create it
              if [ -n "$IPS_NAMES" ]; then
                kubectl patch sa default -n "$ns" --type='json' -p='[{"op":"add","path":"/imagePullSecrets/-","value":{"name":"registry-credentials"}}]' >/dev/null 2>&1 || true
              else
                kubectl patch sa default -n "$ns" --type='json' -p='[{"op":"add","path":"/imagePullSecrets","value":[{"name":"registry-credentials"}]}]' >/dev/null 2>&1 || true
              fi
              echo "  - Patched default ServiceAccount to reference imagePullSecrets: registry-credentials"
            fi
          }
          
          # List of namespaces that need registry-credentials
          # These are application namespaces that pull images from Docker Hub
          NAMESPACES="infra erp truload cafe treasury notifications auth inventory logistics pos argocd iot isp-billing ordering projects subscription ticketing"
          
          for ns in $NAMESPACES; do
            # Namespace should already exist from previous step
            if ! kubectl get namespace "$ns" >/dev/null 2>&1; then
              echo "‚ö†Ô∏è  Warning: namespace $ns does not exist (should have been created earlier)"
              kubectl create namespace "$ns" >/dev/null 2>&1 || true
            fi
            # Create or update secret
            if kubectl get secret registry-credentials -n "$ns" >/dev/null 2>&1; then
              echo "‚úì registry-credentials already exists in namespace: $ns"
              kubectl create secret docker-registry registry-credentials \
                --docker-server="${REGISTRY_SERVER}" \
                --docker-username="${REGISTRY_USERNAME}" \
                --docker-password="${REGISTRY_PASSWORD}" \
                --docker-email="${REGISTRY_EMAIL}" \
                -n "$ns" \
                --dry-run=client -o yaml | kubectl apply -f - || true
              echo "  Updated registry-credentials in namespace: $ns"
            else
              kubectl create secret docker-registry registry-credentials \
                --docker-server="${REGISTRY_SERVER}" \
                --docker-username="${REGISTRY_USERNAME}" \
                --docker-password="${REGISTRY_PASSWORD}" \
                --docker-email="${REGISTRY_EMAIL}" \
                -n "$ns" >/dev/null 2>&1 || true
              echo "‚úì Created registry-credentials in namespace: $ns"
            fi
            # Ensure default service account references the secret (fallback if charts don't set pullSecrets)
            patch_default_sa "$ns"
          done
          
          # Also create in any existing namespaces that might need it
          echo ""
          echo "Checking for other namespaces that might need registry-credentials..."
          EXISTING_NS=$(kubectl get namespaces -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo "")
          
          for ns in $EXISTING_NS; do
            # Skip system namespaces
            if [[ "$ns" == "kube-system" || "$ns" == "kube-public" || "$ns" == "kube-node-lease" || "$ns" == "default" || "$ns" == "calico-system" || "$ns" == "cert-manager" || "$ns" == "ingress-nginx" || "$ns" == "argocd" ]]; then
              continue
            fi
            
            # Skip if already processed above
            if echo "$NAMESPACES" | grep -q "\b$ns\b"; then
              continue
            fi
            
            # Check if namespace has pods that reference registry-credentials
            HAS_REGISTRY_REF=$(kubectl get pods -n "$ns" -o json 2>/dev/null | \
              jq -r '.items[]?.spec.imagePullSecrets[]?.name // empty' 2>/dev/null | \
              grep -q "registry-credentials" && echo "yes" || echo "no")
            
            if [ "$HAS_REGISTRY_REF" = "yes" ]; then
              if ! kubectl get secret registry-credentials -n "$ns" >/dev/null 2>&1; then
                kubectl create secret docker-registry registry-credentials \
                  --docker-server="${REGISTRY_SERVER}" \
                  --docker-username="${REGISTRY_USERNAME}" \
                  --docker-password="${REGISTRY_PASSWORD}" \
                  --docker-email="${REGISTRY_EMAIL}" \
                  -n "$ns" || true
                echo "‚úì Created registry-credentials in namespace: $ns (detected imagePullSecrets reference)"
              fi
              # Ensure default service account references the secret
              patch_default_sa "$ns"
            fi
          done
          
          echo "‚úÖ Registry credentials setup complete"
          echo "::endgroup::"

      - name: Check cluster health and readiness
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        run: |
          echo "::group::Cluster Health Check"
          echo "Checking cluster status..."
          
          # Check nodes
          kubectl get nodes || true
          READY_NODES=$(kubectl get nodes --no-headers 2>/dev/null | grep -c "Ready" || echo "0")
          TOTAL_NODES=$(kubectl get nodes --no-headers 2>/dev/null | wc -l || echo "0")
          
          if [ "$READY_NODES" -eq 0 ] && [ "$TOTAL_NODES" -gt 0 ]; then
            echo "‚ö†Ô∏è  Warning: Cluster has nodes but none are Ready"
            echo "This may indicate CNI or other initialization issues"
          elif [ "$READY_NODES" -gt 0 ]; then
            echo "‚úÖ Cluster is healthy (${READY_NODES}/${TOTAL_NODES} nodes Ready)"
          fi
          
          # Check for Pending pods and run cleanup
          PENDING_COUNT=$(kubectl get pods -A --field-selector=status.phase=Pending --no-headers 2>/dev/null | wc -l || echo "0")
          if [ "$PENDING_COUNT" -gt 0 ]; then
            echo "‚ö†Ô∏è  Warning: Found $PENDING_COUNT Pending pods"
            echo "Running cleanup and diagnostics..."
            
            # First clean up failed/stuck pods
            if [ -f ./scripts/tools/cleanup-failed-pods.sh ]; then
              chmod +x ./scripts/tools/cleanup-failed-pods.sh
              ./scripts/tools/cleanup-failed-pods.sh || true
            fi
            
            # Then run diagnostics
            if [ -f ./scripts/tools/fix-cluster-issues.sh ]; then
              chmod +x ./scripts/tools/fix-cluster-issues.sh
              ./scripts/tools/fix-cluster-issues.sh || true
            fi
          else
            echo "‚úÖ No Pending pods found"
          fi
          
          # Check for CrashLoopBackOff pods
          CRASHING_COUNT=$(kubectl get pods -A --field-selector=status.phase=Failed --no-headers 2>/dev/null | wc -l || echo "0")
          CRASHLOOP_COUNT=$(kubectl get pods -A -o json 2>/dev/null | jq -r '.items[] | select(.status.containerStatuses[]?.state.waiting?.reason == "CrashLoopBackOff") | .metadata.name' 2>/dev/null | wc -l || echo "0")
          if [ "$CRASHING_COUNT" -gt 0 ] || [ "$CRASHLOOP_COUNT" -gt 0 ]; then
            echo "‚ö†Ô∏è  Warning: Found $CRASHING_COUNT Failed pods and $CRASHLOOP_COUNT CrashLoopBackOff pods"
            kubectl get pods -A --field-selector=status.phase=Failed 2>/dev/null | head -5 || true
            echo ""
            echo "Running cleanup and diagnostics..."
            
            # Clean up failed pods first
            if [ -f ./scripts/tools/cleanup-failed-pods.sh ]; then
              chmod +x ./scripts/tools/cleanup-failed-pods.sh
              ./scripts/tools/cleanup-failed-pods.sh || true
            fi
            
            # Then run diagnostics
            if [ -f ./scripts/tools/fix-cluster-issues.sh ]; then
              chmod +x ./scripts/tools/fix-cluster-issues.sh
              ./scripts/tools/fix-cluster-issues.sh || true
            fi
          fi
          
          echo "::endgroup::"

      - name: Check and optimize etcd space (prevent space issues)
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        run: |
          echo "::group::Checking and Optimizing etcd Space"
          
          # Check etcd space before provisioning (prevent space exceeded errors)
          if [ -f ./scripts/cluster/check-etcd-space.sh ]; then
            echo "Checking etcd space status..."
            chmod +x ./scripts/cluster/check-etcd-space.sh
            ./scripts/cluster/check-etcd-space.sh || echo "‚ö†Ô∏è  etcd check completed"
          else
            echo "‚ö†Ô∏è  etcd check script not found - skipping space check"
          fi
          
          # Run fix script if space issues detected (idempotent - only fixes if needed)
          if [ -f ./scripts/cluster/fix-etcd-space.sh ]; then
            echo "Running preventive etcd maintenance (if needed)..."
            chmod +x ./scripts/cluster/fix-etcd-space.sh
            ./scripts/cluster/fix-etcd-space.sh || echo "‚ö†Ô∏è  etcd maintenance completed"
          else
            echo "‚ö†Ô∏è  etcd fix script not found - skipping preventive maintenance"
            echo "‚ö†Ô∏è  If you encounter 'database space exceeded' errors, configure auto-compaction"
            echo "‚ö†Ô∏è  See: docs/ETCD-OPTIMIZATION.md"
          fi
          
          echo "‚úÖ etcd space check complete"
          echo "::endgroup::"

      - name: Install storage provisioner (idempotent)
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        timeout-minutes: 10
        run: |
          echo "::group::Installing Storage Provisioner"
          echo "This step is required before installing databases (for PVCs)"
          ./scripts/infrastructure/install-storage-provisioner.sh
          echo "‚úÖ Storage provisioner installed"
          echo "::endgroup::"

      - name: Configure ingress controller (idempotent)
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        timeout-minutes: 10
        run: |
          echo "::group::Configuring NGINX Ingress Controller"
          echo "Installing NGINX Ingress Controller (required for cert-manager, ArgoCD, and monitoring)"
          ./scripts/infrastructure/configure-ingress-controller.sh
          echo "‚úÖ Ingress controller configured"
          echo "::endgroup::"

      - name: Install cert-manager (idempotent)
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        timeout-minutes: 10
        run: |
          echo "::group::Installing cert-manager"
          echo "Installing cert-manager for TLS certificates (required for ArgoCD and monitoring)"
          echo "This step will skip if cert-manager is already installed and healthy"
          ./scripts/infrastructure/install-cert-manager.sh
          echo "‚úÖ cert-manager check/install complete"
          echo "::endgroup::"

      - name: Clean up orphaned monitoring resources (aggressive)
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        run: |
          echo "::group::Cleaning orphaned monitoring resources"
          MONITORING_NS="${MONITORING_NAMESPACE:-infra}"

          echo "Target monitoring namespace: ${MONITORING_NS}"

          # 1) Clean up failed/orphaned pods cluster-wide (forced)
          if [ -f ./scripts/tools/cleanup-failed-pods.sh ]; then
            chmod +x ./scripts/tools/cleanup-failed-pods.sh
            ./scripts/tools/cleanup-failed-pods.sh || echo "‚ö†Ô∏è  Pod cleanup completed with warnings"
          else
            echo "‚ö†Ô∏è  cleanup-failed-pods.sh not found, skipping pod cleanup"
          fi

          # 2) Remove obsolete monitoring namespace if present (monitoring stack is deployed to infra)
          if [ "${MONITORING_NS}" != "monitoring" ] && kubectl get ns monitoring >/dev/null 2>&1; then
            echo "‚ö†Ô∏è  Obsolete namespace 'monitoring' detected - deleting to prevent stale resources"
            kubectl delete ns monitoring --wait=false 2>/dev/null || true
          fi

          echo "‚úÖ Orphaned resource cleanup complete"
          echo "::endgroup::"

      - name: Install Monitoring (idempotent)
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        timeout-minutes: 25
        continue-on-error: true  # Allow pipeline to continue - monitoring syncs asynchronously via ArgoCD
        env:
          GRAFANA_DOMAIN: ${{ env.GRAFANA_DOMAIN }}
          MONITORING_NAMESPACE: ${{ env.MONITORING_NAMESPACE }}
          ENABLE_CLEANUP: ${{ env.ENABLE_CLEANUP }}
        run: |
          echo "::group::Installing Monitoring Stack"
          echo "Installing Prometheus + Grafana monitoring stack (infra namespace)"
          echo "This step ensures Prometheus Operator CRDs (ServiceMonitor, etc.) exist before databases install"
          echo "Primary deploy mode is ArgoCD Application (apps/monitoring/app.yaml); Helm CLI is fallback if ArgoCD is unavailable"
          echo "Note: This may take 10-15 minutes for first installation"

          # Run monitoring installation script
          chmod +x ./scripts/monitoring/install-monitoring.sh
          ./scripts/monitoring/install-monitoring.sh || {
            echo "‚ö†Ô∏è  Monitoring installation encountered issues - but continuing with pipeline"
            echo "Monitoring will sync asynchronously via ArgoCD"
            echo "Checking for common problems..."

            # Surface ArgoCD application status (if ArgoCD is installed)
            if kubectl get ns argocd >/dev/null 2>&1; then
              echo "ArgoCD Application status:"
              kubectl get application -n argocd prometheus -o wide 2>/dev/null || true
            fi

            echo "Workload status in ${MONITORING_NAMESPACE}:"
            kubectl get pods,svc,ingress -n "${MONITORING_NAMESPACE}" 2>/dev/null || true

            echo "‚ö†Ô∏è  Monitoring may need manual attention - check ArgoCD dashboard"
            # Exit 0 to allow pipeline to continue
            exit 0
          }

          echo "‚úÖ Monitoring stack check/install complete"
          echo "‚úÖ Prometheus Operator CRDs now available for ServiceMonitor resources"
          echo "::endgroup::"

      - name: Install databases (PostgreSQL & Redis - Shared Infrastructure)
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        timeout-minutes: 20
        env:
          # Master password: POSTGRES_PASSWORD used for all infrastructure by default
          # Fallback to service-specific secrets if POSTGRES_PASSWORD not set
          POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}
          POSTGRES_ADMIN_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}
          REDIS_PASSWORD: ${{ secrets.POSTGRES_PASSWORD || secrets.REDIS_PASSWORD }}
          DB_NAMESPACE: ${{ env.DB_NAMESPACE }}
          ENABLE_CLEANUP: ${{ env.ENABLE_CLEANUP }}
          # Docker registry credentials
          REGISTRY_USERNAME: ${{ secrets.REGISTRY_USERNAME }}
          REGISTRY_PASSWORD: ${{ secrets.REGISTRY_PASSWORD }}
          REGISTRY_EMAIL: ${{ secrets.REGISTRY_EMAIL }}
        run: |
          echo "::group::Installing Shared Databases (PostgreSQL & Redis in ${DB_NAMESPACE} namespace)"
          echo "Installing shared infrastructure databases..."
          echo "Namespace: ${DB_NAMESPACE}"
          echo "PostgreSQL: Using custom CodeVertex postgresql-pgvector manifests"
          echo "Redis: Using Bitnami Helm chart"
          echo "PostgreSQL admin_user will be created for managing per-service databases"
          echo "Passwords from GitHub secrets: POSTGRES_PASSWORD, REDIS_PASSWORD"
          echo "Cleanup mode: ${ENABLE_CLEANUP}"
          echo "Note: This may take 5-10 minutes for first installation"
          export NAMESPACE=${DB_NAMESPACE}
          export PG_DATABASE=postgres
          ./scripts/infrastructure/install-databases.sh
          echo "‚úÖ Shared databases installed in ${DB_NAMESPACE} namespace"
          echo "‚úÖ PostgreSQL: Custom manifests with pgvector support"
          echo "‚úÖ PostgreSQL admin_user created (can create per-service databases)"
          echo "‚úÖ Each service will create its own database during deployment"
          echo "::endgroup::"
      
      - name: Install RabbitMQ (Shared Infrastructure)
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        timeout-minutes: 15
        env:
          # Use POSTGRES_PASSWORD as master password, fallback to RABBITMQ_PASSWORD
          RABBITMQ_PASSWORD: ${{ secrets.POSTGRES_PASSWORD || secrets.RABBITMQ_PASSWORD }}
          RABBITMQ_NAMESPACE: ${{ env.DB_NAMESPACE }}
          ENABLE_CLEANUP: ${{ env.ENABLE_CLEANUP }}
        run: |
          echo "::group::Installing RabbitMQ (Shared Infrastructure in ${RABBITMQ_NAMESPACE} namespace)"
          echo "Installing RabbitMQ as shared infrastructure..."
          echo "Namespace: ${RABBITMQ_NAMESPACE}"
          echo "This step will skip installation if RabbitMQ is already installed and healthy"
          echo "Note: This may take 5-10 minutes for first installation"
          ./scripts/infrastructure/install-rabbitmq.sh
          echo "‚úÖ RabbitMQ check/install complete in ${RABBITMQ_NAMESPACE} namespace"
          echo "‚úÖ All services can use shared RabbitMQ instance"
          echo "::endgroup::"

      - name: Create Superset Database and Secrets (Pre-ArgoCD Bootstrap)
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        timeout-minutes: 10
        env:
          POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}
          NAMESPACE: default
          SUPERSET_NAMESPACE: default
        run: |
          echo "::group::Creating Superset Database and Secrets"
          echo "üìä Creating superset database and user in PostgreSQL..."
          echo "This ensures Superset dependencies are ready before ArgoCD deploys it"
          
          # Create Superset database and user
          ./scripts/infrastructure/create-superset-database.sh
          
          # Create Superset secrets (includes SECRET_KEY, admin password, etc.)
          ./scripts/infrastructure/create-superset-secrets.sh
          
          echo "‚úÖ Superset database and secrets created successfully"
          echo "::endgroup::"
      
      - name: Generate Service JWT/RSA Keys (Pre-ArgoCD Bootstrap)
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        timeout-minutes: 5
        run: |
          echo "::group::Generating JWT/RSA Keys for Services"
          
          # Function to generate RSA keys for a service
          generate_service_keys() {
            local SERVICE=$1
            local NAMESPACE=$2
            local SECRET_NAME="${SERVICE}-token-keys"
            
            echo "Checking $SERVICE in $NAMESPACE namespace..."
            
            # Check if secret already exists
            if kubectl get secret "$SECRET_NAME" -n "$NAMESPACE" >/dev/null 2>&1; then
              echo "‚úì JWT keys for $SERVICE already exist - skipping"
              return 0
            fi
            
            # Create namespace if it doesn't exist
            kubectl create namespace "$NAMESPACE" --dry-run=client -o yaml | kubectl apply -f - >/dev/null 2>&1
            
            echo "üîê Generating 4096-bit RSA key pair for $SERVICE..."
            
            # Generate RSA keys
            openssl genrsa -out /tmp/${SERVICE}-private.pem 4096 2>/dev/null
            openssl rsa -in /tmp/${SERVICE}-private.pem -pubout -out /tmp/${SERVICE}-public.pem 2>/dev/null
            
            # Create Kubernetes secret
            kubectl create secret generic "$SECRET_NAME" -n "$NAMESPACE" \
              --from-file=private.pem=/tmp/${SERVICE}-private.pem \
              --from-file=public.pem=/tmp/${SERVICE}-public.pem
            
            # Cleanup temp files
            rm -f /tmp/${SERVICE}-private.pem /tmp/${SERVICE}-public.pem
            
            echo "‚úÖ JWT keys created for $SERVICE in $NAMESPACE namespace"
          }
          
          # Generate keys for services that require JWT authentication
          generate_service_keys "auth-service" "auth"
          
          # Add more services as needed
          # generate_service_keys "other-service" "other-namespace"
          
          echo "‚úÖ All service JWT/RSA keys verified or created"
          echo "::endgroup::"

      - name: Create Auth API Secrets (OAuth Credentials & Admin Seeding)
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        timeout-minutes: 5
        env:
          POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}
          GOOGLE_CLIENT_ID: ${{ secrets.GOOGLE_CLIENT_ID }}
          GOOGLE_CLIENT_SECRET: ${{ secrets.GOOGLE_CLIENT_SECRET }}
          GITHUB_CLIENT_ID: ${{ secrets.GITHUB_CLIENT_ID }}
          GITHUB_CLIENT_SECRET: ${{ secrets.GITHUB_CLIENT_SECRET }}
          MICROSOFT_CLIENT_ID: ${{ secrets.MICROSOFT_CLIENT_ID }}
          MICROSOFT_CLIENT_SECRET: ${{ secrets.MICROSOFT_CLIENT_SECRET }}
          MICROSOFT_TENANT_ID: ${{ secrets.MICROSOFT_TENANT_ID }}
          GLOBAL_ADMIN_PASSWORD: ${{ secrets.GLOBAL_ADMIN_PASSWORD }}
          GLOBAL_ADMIN_EMAIL: ${{ secrets.GLOBAL_ADMIN_EMAIL }}
          DEFAULT_TENANT_SLUG: ${{ secrets.DEFAULT_TENANT_SLUG }}
        run: |
          echo "::group::Creating Auth API Secrets"

          NAMESPACE="auth"
          SECRET_NAME="auth-api-secrets"

          # Create namespace if it doesn't exist
          kubectl create namespace "$NAMESPACE" --dry-run=client -o yaml | kubectl apply -f - >/dev/null 2>&1

          # Check if secret already exists
          if kubectl get secret "$SECRET_NAME" -n "$NAMESPACE" >/dev/null 2>&1; then
            echo "‚úì Auth API secrets already exist - updating..."
            kubectl delete secret "$SECRET_NAME" -n "$NAMESPACE" >/dev/null 2>&1 || true
          fi

          # Get PostgreSQL URL from existing infrastructure
          PG_HOST="postgresql.infra.svc.cluster.local"
          PG_PORT="5432"
          DB_NAME="auth"
          DB_USER="auth_user"
          POSTGRES_URL="postgresql://${DB_USER}:${POSTGRES_PASSWORD}@${PG_HOST}:${PG_PORT}/${DB_NAME}?sslmode=disable"

          # Generate OAuth state secret if not provided
          OAUTH_STATE_SECRET=$(openssl rand -base64 32 | tr -d "=+/" | cut -c1-32)

          # Create secret with all auth-api credentials
          kubectl create secret generic "$SECRET_NAME" -n "$NAMESPACE" \
            --from-literal=postgresUrl="${POSTGRES_URL}" \
            --from-literal=REDIS_PASSWORD="${POSTGRES_PASSWORD}" \
            --from-literal=OAUTH_STATE_SECRET="${OAUTH_STATE_SECRET}" \
            --from-literal=GOOGLE_CLIENT_ID="${GOOGLE_CLIENT_ID:-}" \
            --from-literal=GOOGLE_CLIENT_SECRET="${GOOGLE_CLIENT_SECRET:-}" \
            --from-literal=GITHUB_CLIENT_ID="${GITHUB_CLIENT_ID:-}" \
            --from-literal=GITHUB_CLIENT_SECRET="${GITHUB_CLIENT_SECRET:-}" \
            --from-literal=MICROSOFT_CLIENT_ID="${MICROSOFT_CLIENT_ID:-}" \
            --from-literal=MICROSOFT_CLIENT_SECRET="${MICROSOFT_CLIENT_SECRET:-}" \
            --from-literal=MICROSOFT_TENANT_ID="${MICROSOFT_TENANT_ID:-common}" \
            --from-literal=SEED_ADMIN_PASSWORD="${GLOBAL_ADMIN_PASSWORD:-ChangeMe123!}" \
            --from-literal=SEED_ADMIN_EMAIL="${GLOBAL_ADMIN_EMAIL:-admin@codevertex.com}" \
            --from-literal=DEFAULT_TENANT_SLUG="${DEFAULT_TENANT_SLUG:-codevertex}"

          # Verify OAuth credentials
          if [ -n "${GOOGLE_CLIENT_ID}" ]; then
            echo "‚úÖ Google OAuth credentials configured"
          else
            echo "‚ö†Ô∏è  Google OAuth credentials not set - set GOOGLE_CLIENT_ID and GOOGLE_CLIENT_SECRET in GitHub secrets"
          fi

          if [ -n "${GITHUB_CLIENT_ID}" ]; then
            echo "‚úÖ GitHub OAuth credentials configured"
          else
            echo "‚ö†Ô∏è  GitHub OAuth credentials not set - set GITHUB_CLIENT_ID and GITHUB_CLIENT_SECRET in GitHub secrets"
          fi

          if [ -n "${MICROSOFT_CLIENT_ID}" ]; then
            echo "‚úÖ Microsoft OAuth credentials configured"
          else
            echo "‚ö†Ô∏è  Microsoft OAuth credentials not set (optional) - set MICROSOFT_CLIENT_ID and MICROSOFT_CLIENT_SECRET in GitHub secrets"
          fi

          # Verify admin seeding credentials
          if [ -n "${GLOBAL_ADMIN_PASSWORD}" ]; then
            echo "‚úÖ Global admin password configured for seeding"
          else
            echo "‚ö†Ô∏è  GLOBAL_ADMIN_PASSWORD not set - using default (ChangeMe123!)"
          fi

          if [ -n "${GLOBAL_ADMIN_EMAIL}" ]; then
            echo "‚úÖ Global admin email configured: ${GLOBAL_ADMIN_EMAIL}"
          else
            echo "‚ö†Ô∏è  GLOBAL_ADMIN_EMAIL not set - using default (admin@codevertex.com)"
          fi

          if [ -n "${DEFAULT_TENANT_SLUG}" ]; then
            echo "‚úÖ Default tenant slug configured: ${DEFAULT_TENANT_SLUG}"
          else
            echo "‚ö†Ô∏è  DEFAULT_TENANT_SLUG not set - using default (codevertex)"
          fi

          echo "‚úÖ Auth API secrets created in $NAMESPACE namespace"
          echo "::endgroup::"

      - name: Install Argo CD (idempotent)
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        timeout-minutes: 15
        env:
          ARGOCD_DOMAIN: ${{ env.ARGOCD_DOMAIN }}
        run: |
          echo "::group::Installing Argo CD"
          echo "Installing ArgoCD GitOps tool (requires cert-manager for TLS)"
          echo "This step will skip if Argo CD is already installed and healthy"
          ./scripts/infrastructure/install-argocd.sh
          echo "‚úÖ ArgoCD check/install complete"
          echo "::endgroup::"

      - name: Bootstrap Argo CD applications (app-of-apps)
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        run: |
          echo "::group::Bootstrapping Argo CD applications"
          echo "This step uses kubectl apply (idempotent) - safe to rerun"
          # Apply the root Application that manages apps under apps/
          if [ -f apps/root-app.yaml ]; then
            kubectl apply -f apps/root-app.yaml
            echo "‚úÖ root Application applied/updated"
          else
            echo "‚ö†Ô∏è apps/root-app.yaml not found; skipping"
          fi
          
          # Also apply individual applications directly to ensure they're created
          echo "Applying individual ArgoCD Applications (idempotent)..."
          for app_file in apps/*/app.yaml; do
            if [ -f "$app_file" ]; then
              kubectl apply -f "$app_file" || echo "‚ö†Ô∏è Failed to apply $app_file"
              echo "‚úì Applied/updated: $app_file"
            fi
          done
          
          echo "Current ArgoCD Applications:"
          kubectl get applications -n argocd || true
          echo "::endgroup::"

      - name: Install Vertical Pod Autoscaler (VPA) [Optional]
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        timeout-minutes: 10
        continue-on-error: true
        run: |
          echo "::group::Installing Vertical Pod Autoscaler (Optional)"
          echo "Installing VPA for automatic pod resource optimization"
          echo "This step is optional and will skip if VPA is already installed"
          echo "Set FORCE_INSTALL=true to force reinstallation"
          ./scripts/infrastructure/install-vpa.sh || echo "‚ö†Ô∏è VPA installation skipped or failed (non-critical)"
          echo "‚úÖ VPA check/install complete"
          echo "::endgroup::"

      - name: Setup Git Access (SSH Keys & Configuration) [Optional]
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        continue-on-error: true
        run: |
          echo "::group::Setting up Git SSH Access (Optional)"
          echo "‚ö†Ô∏è  Note: Git SSH access setup is optional and requires manual GitHub deploy key configuration"
          echo "This step generates a key pair, but you must manually add the public key to GitHub"
          echo "This step will skip if SSH keys already exist"
          echo ""
          
          # Create SSH directory if it doesn't exist
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          
          # Generate SSH key pair for git operations (if not exists)
          if [ ! -f ~/.ssh/git_deploy_key ]; then
            echo "üîë Generating SSH key pair for git operations..."
            ssh-keygen -t ed25519 -C "vps-git-access@bengoerp" -f ~/.ssh/git_deploy_key -N ""
            chmod 600 ~/.ssh/git_deploy_key
            chmod 644 ~/.ssh/git_deploy_key.pub
          else
            echo "‚úÖ SSH key pair already exists"
          fi
          
          # Add GitHub to known hosts
          ssh-keyscan github.com >> ~/.ssh/known_hosts 2>/dev/null || true
          
          # Create SSH config for reliable git operations
          cat > ~/.ssh/config << 'EOF'
          Host github.com
              HostName github.com
              User git
              IdentityFile ~/.ssh/git_deploy_key
              IdentitiesOnly yes
              StrictHostKeyChecking no
          EOF
          chmod 600 ~/.ssh/config
          
          # Display public key for deploy key setup
          echo ""
          echo "üîë PUBLIC KEY FOR GITHUB DEPLOY KEY (MANUAL SETUP REQUIRED):"
          echo "============================================================="
          cat ~/.ssh/git_deploy_key.pub
          echo "============================================================="
          echo ""
          echo "üìã MANUAL STEPS:"
          echo "1. Copy the public key above"
          echo "2. Go to: https://github.com/Bengo-Hub/devops-k8s/settings/keys"
          echo "3. Add deploy key with 'Allow write access' enabled"
          echo "4. Test: git clone git@github.com:Bengo-Hub/devops-k8s.git"
          echo ""
          
          echo "::endgroup::"
      
      - name: Verify Ingress Configuration
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        continue-on-error: true
        run: |
          echo "::group::Verifying Ingress Configuration"
          echo "Checking all ingress resources for proper configuration..."
          if [ -f ./scripts/tools/verify-ingress-config.sh ]; then
            chmod +x ./scripts/tools/verify-ingress-config.sh
            ./scripts/tools/verify-ingress-config.sh || echo "‚ö†Ô∏è  Ingress verification completed with warnings"
          else
            echo "‚ö†Ô∏è  verify-ingress-config.sh not found, skipping verification"
          fi
          echo "::endgroup::"

      - name: Audit Resource Usage
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        continue-on-error: true
        run: |
          echo "::group::Auditing Cluster Resource Usage"
          echo "Checking pod counts and resource allocation..."
          if [ -f ./scripts/tools/audit-resources.sh ]; then
            chmod +x ./scripts/tools/audit-resources.sh
            ./scripts/tools/audit-resources.sh || echo "‚ö†Ô∏è  Resource audit completed with warnings"
          else
            echo "‚ö†Ô∏è  audit-resources.sh not found, skipping audit"
          fi
          echo "::endgroup::"

      - name: Summary
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        env:
          ARGOCD_DOMAIN: ${{ env.ARGOCD_DOMAIN }}
          GRAFANA_DOMAIN: ${{ env.GRAFANA_DOMAIN }}
          DB_NAMESPACE: ${{ env.DB_NAMESPACE }}
        run: |
          echo "::group::Cluster Infrastructure Provisioning Complete"
          echo ""
          echo "‚úÖ Storage provisioner installed"
          echo "‚úÖ Shared Databases installed (PostgreSQL & Redis in ${DB_NAMESPACE} namespace)"
          echo "  - PostgreSQL admin_user created for managing per-service databases"
          echo "  - Each service creates its own database during deployment"
          echo "‚úÖ RabbitMQ installed (shared infrastructure in ${DB_NAMESPACE} namespace)"
          echo "‚úÖ NGINX Ingress Controller configured"
          echo "‚úÖ cert-manager installed"
          echo "‚úÖ Argo CD installed (https://${ARGOCD_DOMAIN})"
          echo "‚úÖ Monitoring Stack installed (https://${GRAFANA_DOMAIN}) in ${DB_NAMESPACE} namespace"
          echo "‚úÖ Vertical Pod Autoscaler (VPA) installed"
          echo "‚úÖ Ingress configuration verified"
          echo "‚úÖ Resource usage audited"
          echo ""
          echo "Shared Infrastructure Credentials (${DB_NAMESPACE} namespace):"
          echo "- PostgreSQL admin_user: kubectl get secret postgresql -n ${DB_NAMESPACE} -o jsonpath='{.data.admin-user-password}' | base64 -d"
          echo "- PostgreSQL postgres user: kubectl get secret postgresql -n ${DB_NAMESPACE} -o jsonpath='{.data.postgres-password}' | base64 -d"
          echo "- Redis: kubectl get secret redis -n ${DB_NAMESPACE} -o jsonpath='{.data.redis-password}' | base64 -d"
          echo "- RabbitMQ: kubectl get secret rabbitmq -n ${DB_NAMESPACE} -o jsonpath='{.data.rabbitmq-password}' | base64 -d"
          echo ""
          echo "Per-Service Databases:"
          echo "- Each service (cafe-backend, erp-api, treasury-app, notifications-app) creates its own database"
          echo "- Database names: cafe, bengo_erp, treasury, notifications"
          echo "- Databases are created automatically during service deployment"
          echo ""
          echo "Configuration (via GitHub Secrets - priority order):"
          echo "- ARGOCD_DOMAIN: ${ARGOCD_DOMAIN} (from secrets.ARGOCD_DOMAIN or default)"
          echo "- GRAFANA_DOMAIN: ${GRAFANA_DOMAIN} (from secrets.GRAFANA_DOMAIN or default)"
          echo "- DB_NAMESPACE: ${DB_NAMESPACE} (from secrets.DB_NAMESPACE or default)"
          echo ""
          echo "Next steps:"
          echo "1. Set up GitHub deploy key (see Git SSH Access step above - manual setup required)"
          echo "2. Point DNS to your VPS IP"
          echo "3. Configure Argo CD repository access"
          echo "4. Deploy applications via Argo CD (they will auto-create databases)"
          echo "5. Verify databases: kubectl -n ${DB_NAMESPACE} exec -it postgresql-0 -- psql -U admin_user -d postgres -c '\l'"
          echo "6. Verify VPA: kubectl get vpa --all-namespaces"
          echo ""
          echo "üìö Documentation:"
          echo "- Manual cluster setup: docs/contabo-setup-kubeadm.md"
          echo "- End-to-end workflow: docs/CLUSTER-SETUP-WORKFLOW.md"
          echo "- GitHub secrets: docs/github-secrets.md"
          echo "- Provisioning guide: docs/provisioning.md"
          echo "::endgroup::"

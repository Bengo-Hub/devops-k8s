name: Provision Cluster Infrastructure

on:
  push:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      enable_cleanup:
        description: 'Enable cluster cleanup (DANGEROUS - deletes all data). Default: false (disabled). Set to true to enable.'
        required: false
        default: false
        type: boolean
      force_reprovision:
        description: 'Force reprovisioning (cleanup + reinstall). Requires enable_cleanup=true.'
        required: false
        default: false
        type: boolean

jobs:
  provision:
    runs-on: ubuntu-latest
    permissions:
      contents: read
    env:
      # Configurable domains and IPs via GitHub secrets (with defaults)
      # Priority: GitHub secrets first, fallback to defaults
      ARGOCD_DOMAIN: ${{ secrets.ARGOCD_DOMAIN || 'argocd.masterspace.co.ke' }}
      GRAFANA_DOMAIN: ${{ secrets.GRAFANA_DOMAIN || 'grafana.masterspace.co.ke' }}
      DB_NAMESPACE: ${{ secrets.DB_NAMESPACE || 'infra' }}
      MONITORING_NAMESPACE: ${{ secrets.MONITORING_NAMESPACE || 'infra' }}
      CLUSTER_NAME: ${{ secrets.CLUSTER_NAME || 'mss-prod' }}
      # Cleanup disabled by default, can be enabled via workflow input OR GitHub secret
      # Priority: workflow input > GitHub secret > default false
      ENABLE_CLEANUP: ${{ github.event_name == 'workflow_dispatch'
        && (github.event.inputs.enable_cleanup == 'true' && 'true' || 'false')
        || (secrets.ENABLE_CLEANUP == 'true' && 'true' || 'false') }}
      FORCE_REPROVISION: ${{ github.event_name == 'workflow_dispatch'
        && (github.event.inputs.force_reprovision == 'true' && 'true' || 'false')
        || (secrets.FORCE_REPROVISION == 'true' && 'true' || 'false') }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install DevOps tools (kubectl, helm, yq, argocd)
        uses: ./.github/actions/install-devops-tools
        with:
          install_trivy: 'false'

      - name: Install PostgreSQL client (psql) [idempotent]
        run: |
          echo "::group::Installing PostgreSQL client"
          # Check if packages are already installed
          MISSING_PKGS=()
          
          # Check for PostgreSQL client (psql command)
          if ! command -v psql &> /dev/null; then
            MISSING_PKGS+=("postgresql-client" "postgresql-client-common")
          else
            echo "‚úì PostgreSQL client (psql) already installed"
            psql --version
          fi
          
          # Note: jq and curl are already installed by install-devops-tools action
          # Only install if missing (defensive check)
          if ! command -v jq &> /dev/null; then
            MISSING_PKGS+=("jq")
          fi
          
          if ! command -v curl &> /dev/null; then
            MISSING_PKGS+=("curl")
          fi
          
          # Only run apt-get if packages are missing
          if [ ${#MISSING_PKGS[@]} -eq 0 ]; then
            echo "‚úì All required packages already installed"
            echo "::endgroup::"
            exit 0
          fi
          
          echo "Installing missing packages: ${MISSING_PKGS[*]}"
          sudo apt-get update -y
          sudo apt-get install -y "${MISSING_PKGS[@]}"
          
          # Verify installation
          if command -v psql &> /dev/null; then
            echo "‚úì PostgreSQL client installed successfully"
            psql --version
          fi
          echo "::endgroup::"

      - name: Get VPS IP and ensure running (via Contabo API)
        id: contabo_vps
        env:
          CONTABO_CLIENT_ID: ${{ secrets.CONTABO_CLIENT_ID }}
          CONTABO_CLIENT_SECRET: ${{ secrets.CONTABO_CLIENT_SECRET }}
          CONTABO_API_USERNAME: ${{ secrets.CONTABO_API_USERNAME }}
          CONTABO_API_PASSWORD: ${{ secrets.CONTABO_API_PASSWORD }}
          CONTABO_INSTANCE_ID: ${{ secrets.CONTABO_INSTANCE_ID || '14285715' }}
          SSH_HOST: ${{ secrets.SSH_HOST || '' }}
        run: |
          echo "::group::Contabo API - VPS Management"
          
          # Priority: SSH_HOST secret > Contabo API
          if [ -n "${SSH_HOST}" ] && [ "${SSH_HOST}" != "YOUR_VPS_IP" ]; then
            echo "‚úÖ Using VPS IP from SSH_HOST secret: ${SSH_HOST}"
            echo "vps_ip=${SSH_HOST}" >> $GITHUB_OUTPUT
            echo "vps_source=github_secret" >> $GITHUB_OUTPUT
            echo "::endgroup::"
            exit 0
          fi
            
            # Check if Contabo API credentials are available
            if [ -z "${CONTABO_CLIENT_ID}" ] || [ -z "${CONTABO_CLIENT_SECRET}" ] || \
               [ -z "${CONTABO_API_USERNAME}" ] || [ -z "${CONTABO_API_PASSWORD}" ]; then
            echo "‚ö†Ô∏è  Contabo API credentials not available"
            echo "‚ö†Ô∏è  Please set SSH_HOST secret with VPS IP address"
              echo "‚ö†Ô∏è  Or configure Contabo API credentials (CONTABO_CLIENT_ID, CONTABO_CLIENT_SECRET, etc.)"
            echo "vps_ip=" >> $GITHUB_OUTPUT
            echo "vps_source=none" >> $GITHUB_OUTPUT
              echo "::endgroup::"
              exit 0
            fi
            
          # Source Contabo API helper
            source ./scripts/tools/contabo-api.sh
            
          # Get access token
          ACCESS_TOKEN=$(get_contabo_token)
          if [ $? -ne 0 ]; then
            echo "‚ùå Failed to get Contabo API token"
            echo "vps_ip=" >> $GITHUB_OUTPUT
            echo "vps_source=none" >> $GITHUB_OUTPUT
            echo "::endgroup::"
            exit 0
          fi
          
          # Ensure VPS is running
          echo "Ensuring VPS is running..."
          ensure_vps_running "$ACCESS_TOKEN"
          
          # Get VPS IP
          VPS_IP=$(get_vps_ip_from_contabo "$ACCESS_TOKEN")
          if [ $? -eq 0 ] && [ -n "$VPS_IP" ]; then
            echo "‚úÖ Got VPS IP from Contabo API: ${VPS_IP}"
            echo "vps_ip=${VPS_IP}" >> $GITHUB_OUTPUT
            echo "vps_source=contabo_api" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Failed to get VPS IP from Contabo API"
            echo "vps_ip=" >> $GITHUB_OUTPUT
            echo "vps_source=none" >> $GITHUB_OUTPUT
          fi
          
          echo "::endgroup::"

      - name: Configure kubeconfig
        id: kubeconfig_setup
        env:
          KUBE_CONFIG_B64: ${{ secrets.KUBE_CONFIG }}
        run: |
          echo "::group::Checking KUBE_CONFIG"
          if [ -z "$KUBE_CONFIG_B64" ]; then
            echo "‚ùå KUBE_CONFIG secret not provided."
            echo ""
            echo "üìã MANUAL SETUP REQUIRED:"
            echo "This workflow requires a Kubernetes cluster to be set up manually first."
            echo "Please follow the manual setup guide: docs/contabo-setup-kubeadm.md"
            echo ""
            echo "After completing manual setup:"
            echo "1. Get your kubeconfig from the VPS: cat ~/.kube/config | base64 -w 0"
            echo "2. Add it as GitHub secret: KUBE_CONFIG"
            echo "3. Re-run this workflow to provision infrastructure"
            echo ""
            echo "skip_provision=true" >> $GITHUB_ENV
            echo "kubeconfig_ready=false" >> $GITHUB_OUTPUT
            echo "::endgroup::"
            exit 0
          fi
          echo "‚úÖ KUBE_CONFIG secret found"
          echo "::endgroup::"
          
          echo "::group::Decoding and configuring kubeconfig"
          mkdir -p ~/.kube
          
          # Clean the base64 string (remove whitespace, newlines, etc.)
          CLEAN_B64=$(echo "$KUBE_CONFIG_B64" | tr -d '[:space:]')
          
          # Validate base64 format
          if ! echo "$CLEAN_B64" | base64 -d >/dev/null 2>&1; then
            echo "‚ùå Invalid base64 format in KUBE_CONFIG secret"
            echo ""
            echo "Troubleshooting:"
            echo "1. Make sure you copied the ENTIRE base64 output from setup-cluster.sh"
            echo "2. The kubeconfig should be base64-encoded WITHOUT line breaks"
            echo "3. To regenerate on VPS: cat /etc/kubernetes/admin.conf | base64 -w 0"
            echo "4. Or: cat ~/.kube/config | base64 -w 0"
            echo ""
            echo "Current secret length: ${#CLEAN_B64} characters"
            echo "First 50 chars: ${CLEAN_B64:0:50}..."
            exit 1
          fi
          
          # Decode and write kubeconfig
          echo "$CLEAN_B64" | base64 -d > ~/.kube/config
          
          # Verify kubeconfig file was created and has content
          if [ ! -s ~/.kube/config ]; then
            echo "‚ùå Kubeconfig file is empty after decoding"
            exit 1
          fi
          
          chmod 600 ~/.kube/config
          echo "‚úÖ Kubeconfig written to ~/.kube/config ($(wc -c < ~/.kube/config) bytes)"
          echo "::endgroup::"
          
          echo "::group::Testing cluster connectivity"
          if kubectl cluster-info >/dev/null 2>&1; then
            echo "‚úÖ Successfully connected to Kubernetes cluster"
            kubectl get nodes || true
            
            # Check if cluster is ready for provisioning
            READY_NODES=$(kubectl get nodes --no-headers 2>/dev/null | grep -c "Ready" || echo "0")
            if [ "$READY_NODES" -gt 0 ]; then
              echo "‚úÖ Cluster is ready (${READY_NODES} node(s) Ready)"
              echo "skip_provision=false" >> $GITHUB_ENV
              echo "kubeconfig_ready=true" >> $GITHUB_OUTPUT
            else
              echo "‚ö†Ô∏è  No Ready nodes found. Cluster may not be fully initialized."
            echo "skip_provision=false" >> $GITHUB_ENV
              echo "kubeconfig_ready=true" >> $GITHUB_OUTPUT
            fi
          else
            echo "‚ùå Failed to connect to cluster. Check your KUBE_CONFIG."
            echo "skip_provision=true" >> $GITHUB_ENV
            echo "kubeconfig_ready=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          echo "::endgroup::"

      - name: Ensure scripts executable
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        run: |
          echo "::group::Making scripts executable"
          find scripts -type f -name "*.sh" -exec chmod +x {} \;
          echo "‚úÖ All scripts are now executable"
          echo "::endgroup::"

      - name: Cleanup cluster (opt-in only)
        if: ${{ steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true' && env.ENABLE_CLEANUP == 'true' }}
        env:
          ENABLE_CLEANUP: ${{ env.ENABLE_CLEANUP }}
          FORCE_CLEANUP: 'true'
        run: |
          echo "::group::Cluster Cleanup & Reprovisioning"
          echo "ENABLE_CLEANUP=${ENABLE_CLEANUP}"
          echo "FORCE_CLEANUP=${FORCE_CLEANUP}"
          echo "üßπ Cleanup is ENABLED - This will delete ALL applications and data!"
          echo "‚ö†Ô∏è  Deleting namespaces: erp, truload, infra, argocd, monitoring, cafe, treasury, notifications, auth-service"
          echo "‚ö†Ô∏è  System namespaces preserved: kube-system, calico-system, cert-manager, ingress-nginx"
          export ENABLE_CLEANUP=true
          export FORCE_CLEANUP=true
          ./scripts/cluster/cleanup-cluster.sh
          echo "‚úÖ Cleanup complete"
          echo "::endgroup::"

      - name: Create registry-credentials secrets in all namespaces
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        env:
          REGISTRY_USERNAME: ${{ secrets.REGISTRY_USERNAME || secrets.DOCKER_USERNAME || secrets.DOCKERHUB_USERNAME || 'codevertex' }}
          REGISTRY_PASSWORD: ${{ secrets.REGISTRY_PASSWORD || secrets.DOCKER_PASSWORD || secrets.DOCKERHUB_TOKEN || secrets.DOCKERHUB_PASSWORD }}
          REGISTRY_EMAIL: ${{ secrets.REGISTRY_EMAIL || secrets.DOCKER_EMAIL || 'codevertexitsolutions@gmail.com' }}
          REGISTRY_SERVER: docker.io
        run: |
          echo "::group::Creating registry-credentials secrets"
          
          if [ -z "${REGISTRY_PASSWORD}" ]; then
            echo "‚ö†Ô∏è  REGISTRY_PASSWORD not set - skipping registry secret creation"
            echo "‚ö†Ô∏è  Pods may fail with ImagePullBackOff if Docker Hub rate limit is exceeded"
            echo "‚ö†Ô∏è  Set REGISTRY_PASSWORD GitHub secret to enable Docker Hub authentication"
            echo "::endgroup::"
            exit 0
          fi
          
          echo "Using registry credentials:"
          echo "  Username: ${REGISTRY_USERNAME}"
          echo "  Email: ${REGISTRY_EMAIL}"
          echo "  Server: ${REGISTRY_SERVER}"
          
          patch_default_sa() {
            local ns="$1"
            # Ensure default service account exists (it should)
            if ! kubectl get sa default -n "$ns" >/dev/null 2>&1; then
              return 0
            fi
            # Check if imagePullSecrets already includes registry-credentials
            IPS_NAMES=$(kubectl get sa default -n "$ns" -o jsonpath='{.imagePullSecrets[*].name}' 2>/dev/null || echo "")
            if echo "$IPS_NAMES" | grep -qw "registry-credentials"; then
              echo "  - default ServiceAccount already references imagePullSecrets: registry-credentials"
            else
              # If imagePullSecrets array exists, append; else create it
              if [ -n "$IPS_NAMES" ]; then
                kubectl patch sa default -n "$ns" --type='json' -p='[{"op":"add","path":"/imagePullSecrets/-","value":{"name":"registry-credentials"}}]' >/dev/null 2>&1 || true
              else
                kubectl patch sa default -n "$ns" --type='json' -p='[{"op":"add","path":"/imagePullSecrets","value":[{"name":"registry-credentials"}]}]' >/dev/null 2>&1 || true
              fi
              echo "  - Patched default ServiceAccount to reference imagePullSecrets: registry-credentials"
            fi
          }
          
          # List of namespaces that need registry-credentials
          # These are application namespaces that pull images from Docker Hub
          NAMESPACES="infra erp truload cafe treasury notifications auth inventory logistics pos argocd"
          
          for ns in $NAMESPACES; do
            # Ensure namespace exists (to avoid race conditions with ArgoCD)
            if ! kubectl get namespace "$ns" >/dev/null 2>&1; then
              kubectl create namespace "$ns" >/dev/null 2>&1 || true
              echo "‚úì Created namespace: $ns"
            fi
            # Create or update secret
            if kubectl get secret registry-credentials -n "$ns" >/dev/null 2>&1; then
              echo "‚úì registry-credentials already exists in namespace: $ns"
              kubectl create secret docker-registry registry-credentials \
                --docker-server="${REGISTRY_SERVER}" \
                --docker-username="${REGISTRY_USERNAME}" \
                --docker-password="${REGISTRY_PASSWORD}" \
                --docker-email="${REGISTRY_EMAIL}" \
                -n "$ns" \
                --dry-run=client -o yaml | kubectl apply -f - || true
              echo "  Updated registry-credentials in namespace: $ns"
            else
              kubectl create secret docker-registry registry-credentials \
                --docker-server="${REGISTRY_SERVER}" \
                --docker-username="${REGISTRY_USERNAME}" \
                --docker-password="${REGISTRY_PASSWORD}" \
                --docker-email="${REGISTRY_EMAIL}" \
                -n "$ns" >/dev/null 2>&1 || true
              echo "‚úì Created registry-credentials in namespace: $ns"
            fi
            # Ensure default service account references the secret (fallback if charts don't set pullSecrets)
            patch_default_sa "$ns"
          done
          
          # Also create in any existing namespaces that might need it
          echo ""
          echo "Checking for other namespaces that might need registry-credentials..."
          EXISTING_NS=$(kubectl get namespaces -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo "")
          
          for ns in $EXISTING_NS; do
            # Skip system namespaces
            if [[ "$ns" == "kube-system" || "$ns" == "kube-public" || "$ns" == "kube-node-lease" || "$ns" == "default" || "$ns" == "calico-system" || "$ns" == "cert-manager" || "$ns" == "ingress-nginx" || "$ns" == "argocd" ]]; then
              continue
            fi
            
            # Skip if already processed above
            if echo "$NAMESPACES" | grep -q "\b$ns\b"; then
              continue
            fi
            
            # Check if namespace has pods that reference registry-credentials
            HAS_REGISTRY_REF=$(kubectl get pods -n "$ns" -o json 2>/dev/null | \
              jq -r '.items[]?.spec.imagePullSecrets[]?.name // empty' 2>/dev/null | \
              grep -q "registry-credentials" && echo "yes" || echo "no")
            
            if [ "$HAS_REGISTRY_REF" = "yes" ]; then
              if ! kubectl get secret registry-credentials -n "$ns" >/dev/null 2>&1; then
                kubectl create secret docker-registry registry-credentials \
                  --docker-server="${REGISTRY_SERVER}" \
                  --docker-username="${REGISTRY_USERNAME}" \
                  --docker-password="${REGISTRY_PASSWORD}" \
                  --docker-email="${REGISTRY_EMAIL}" \
                  -n "$ns" || true
                echo "‚úì Created registry-credentials in namespace: $ns (detected imagePullSecrets reference)"
              fi
              # Ensure default service account references the secret
              patch_default_sa "$ns"
            fi
          done
          
          echo "‚úÖ Registry credentials setup complete"
          echo "::endgroup::"

      - name: Check cluster health and readiness
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        run: |
          echo "::group::Cluster Health Check"
          echo "Checking cluster status..."
          
          # Check nodes
          kubectl get nodes || true
          READY_NODES=$(kubectl get nodes --no-headers 2>/dev/null | grep -c "Ready" || echo "0")
          TOTAL_NODES=$(kubectl get nodes --no-headers 2>/dev/null | wc -l || echo "0")
          
          if [ "$READY_NODES" -eq 0 ] && [ "$TOTAL_NODES" -gt 0 ]; then
            echo "‚ö†Ô∏è  Warning: Cluster has nodes but none are Ready"
            echo "This may indicate CNI or other initialization issues"
          elif [ "$READY_NODES" -gt 0 ]; then
            echo "‚úÖ Cluster is healthy (${READY_NODES}/${TOTAL_NODES} nodes Ready)"
          fi
          
          # Check for Pending pods
          PENDING_COUNT=$(kubectl get pods -A --field-selector=status.phase=Pending --no-headers 2>/dev/null | wc -l || echo "0")
          if [ "$PENDING_COUNT" -gt 0 ]; then
            echo "‚ö†Ô∏è  Warning: Found $PENDING_COUNT Pending pods"
            echo "Running diagnostics..."
            if [ -f ./scripts/tools/fix-cluster-issues.sh ]; then
              chmod +x ./scripts/tools/fix-cluster-issues.sh
              ./scripts/tools/fix-cluster-issues.sh || true
            elif [ -f ./scripts/diagnostics/diagnose-pending-pods.sh ]; then
              chmod +x ./scripts/diagnostics/diagnose-pending-pods.sh
              ./scripts/diagnostics/diagnose-pending-pods.sh || true
            fi
          else
            echo "‚úÖ No Pending pods found"
          fi
          
          # Check for CrashLoopBackOff pods
          CRASHING_COUNT=$(kubectl get pods -A --field-selector=status.phase=Failed --no-headers 2>/dev/null | wc -l || echo "0")
          CRASHLOOP_COUNT=$(kubectl get pods -A -o json 2>/dev/null | jq -r '.items[] | select(.status.containerStatuses[]?.state.waiting?.reason == "CrashLoopBackOff") | .metadata.name' 2>/dev/null | wc -l || echo "0")
          if [ "$CRASHING_COUNT" -gt 0 ] || [ "$CRASHLOOP_COUNT" -gt 0 ]; then
            echo "‚ö†Ô∏è  Warning: Found $CRASHING_COUNT Failed pods and $CRASHLOOP_COUNT CrashLoopBackOff pods"
            kubectl get pods -A --field-selector=status.phase=Failed 2>/dev/null | head -5 || true
            echo ""
            echo "Running diagnostics for crashing pods..."
            if [ -f ./scripts/tools/fix-cluster-issues.sh ]; then
              chmod +x ./scripts/tools/fix-cluster-issues.sh
              ./scripts/tools/fix-cluster-issues.sh || true
            fi
          fi
          
          echo "::endgroup::"

      - name: Check and optimize etcd space (prevent space issues)
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        run: |
          echo "::group::Checking and Optimizing etcd Space"
          
          # Check etcd space before provisioning (prevent space exceeded errors)
          if [ -f ./scripts/cluster/check-etcd-space.sh ]; then
            echo "Checking etcd space status..."
            chmod +x ./scripts/cluster/check-etcd-space.sh
            ./scripts/cluster/check-etcd-space.sh || echo "‚ö†Ô∏è  etcd check completed"
          else
            echo "‚ö†Ô∏è  etcd check script not found - skipping space check"
          fi
          
          # Run fix script if space issues detected (idempotent - only fixes if needed)
          if [ -f ./scripts/cluster/fix-etcd-space.sh ]; then
            echo "Running preventive etcd maintenance (if needed)..."
            chmod +x ./scripts/cluster/fix-etcd-space.sh
            ./scripts/cluster/fix-etcd-space.sh || echo "‚ö†Ô∏è  etcd maintenance completed"
          else
            echo "‚ö†Ô∏è  etcd fix script not found - skipping preventive maintenance"
            echo "‚ö†Ô∏è  If you encounter 'database space exceeded' errors, configure auto-compaction"
            echo "‚ö†Ô∏è  See: docs/ETCD-OPTIMIZATION.md"
          fi
          
          echo "‚úÖ etcd space check complete"
          echo "::endgroup::"

      - name: Install storage provisioner (idempotent)
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        timeout-minutes: 10
        run: |
          echo "::group::Installing Storage Provisioner"
          echo "This step is required before installing databases (for PVCs)"
          ./scripts/infrastructure/install-storage-provisioner.sh
          echo "‚úÖ Storage provisioner installed"
          echo "::endgroup::"

      - name: Configure ingress controller (idempotent)
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        timeout-minutes: 10
        run: |
          echo "::group::Configuring NGINX Ingress Controller"
          echo "Installing NGINX Ingress Controller (required for cert-manager, ArgoCD, and monitoring)"
          ./scripts/infrastructure/configure-ingress-controller.sh
          echo "‚úÖ Ingress controller configured"
          echo "::endgroup::"

      - name: Install cert-manager (idempotent)
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        timeout-minutes: 10
        run: |
          echo "::group::Installing cert-manager"
          echo "Installing cert-manager for TLS certificates (required for ArgoCD and monitoring)"
          echo "This step will skip if cert-manager is already installed and healthy"
          ./scripts/infrastructure/install-cert-manager.sh
          echo "‚úÖ cert-manager check/install complete"
          echo "::endgroup::"

      - name: Install Monitoring (idempotent)
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        timeout-minutes: 25
        env:
          GRAFANA_DOMAIN: ${{ env.GRAFANA_DOMAIN }}
          MONITORING_NAMESPACE: ${{ env.MONITORING_NAMESPACE }}
          ENABLE_CLEANUP: ${{ env.ENABLE_CLEANUP }}
        run: |
          echo "::group::Installing Monitoring Stack"
          echo "Installing Prometheus + Grafana monitoring stack"
          echo "This installs Prometheus Operator CRDs (ServiceMonitor, etc.) required by databases"
          echo "This step uses helm upgrade --install (idempotent) and will skip if already installed"
          echo "Note: This may take 10-15 minutes for first installation"
          
          # Run monitoring installation script
          chmod +x ./scripts/monitoring/install-monitoring.sh
          ./scripts/monitoring/install-monitoring.sh || {
            echo "‚ö†Ô∏è  Monitoring installation encountered issues"
            echo "Checking for common problems..."
            
            # Check for stuck Helm operations
            if helm status prometheus -n "${MONITORING_NAMESPACE}" 2>/dev/null | grep -q "STATUS: pending"; then
              echo "‚ö†Ô∏è  Detected stuck Helm operation. Attempting fix..."
              if [ -f ./scripts/monitoring/fix-stuck-helm-monitoring.sh ]; then
                chmod +x ./scripts/monitoring/fix-stuck-helm-monitoring.sh
                ./scripts/monitoring/fix-stuck-helm-monitoring.sh || true
              fi
            fi
            
            # Run cluster diagnostics
            if [ -f ./scripts/tools/fix-cluster-issues.sh ]; then
              chmod +x ./scripts/tools/fix-cluster-issues.sh
              ./scripts/tools/fix-cluster-issues.sh || true
            fi
            
            echo "‚ö†Ô∏è  Please review logs above and retry if needed"
            exit 1
          }
          
          echo "‚úÖ Monitoring stack check/install complete"
          echo "‚úÖ Prometheus Operator CRDs now available for ServiceMonitor resources"
          echo "::endgroup::"

      - name: Install databases (PostgreSQL & Redis - Shared Infrastructure)
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        timeout-minutes: 20
        env:
          # Priority: GitHub secrets required (no fallback to auto-generated)
          # Both postgres superuser and admin_user will use POSTGRES_PASSWORD unless POSTGRES_ADMIN_PASSWORD is explicitly set
          POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD || '' }}
          POSTGRES_ADMIN_PASSWORD: ${{ secrets.POSTGRES_ADMIN_PASSWORD || secrets.POSTGRES_PASSWORD || '' }}
          REDIS_PASSWORD: ${{ secrets.REDIS_PASSWORD || '' }}
          DB_NAMESPACE: ${{ env.DB_NAMESPACE }}
          ENABLE_CLEANUP: ${{ env.ENABLE_CLEANUP }}
          # Docker registry credentials for custom PostgreSQL image build
          REGISTRY_USERNAME: ${{ secrets.REGISTRY_USERNAME || secrets.DOCKER_USERNAME || secrets.DOCKERHUB_USERNAME || 'codevertex' }}
          REGISTRY_PASSWORD: ${{ secrets.REGISTRY_PASSWORD || secrets.DOCKER_PASSWORD || secrets.DOCKERHUB_TOKEN || secrets.DOCKERHUB_PASSWORD }}
          REGISTRY_SERVER: docker.io
        run: |
          echo "::group::Installing Shared Databases (PostgreSQL & Redis in ${DB_NAMESPACE} namespace)"
          echo "Installing shared infrastructure databases..."
          echo "Namespace: ${DB_NAMESPACE}"
          echo "PostgreSQL will create admin_user for managing per-service databases"
          echo "ServiceMonitor will be enabled automatically if Prometheus Operator CRDs are available"
          echo "This step will skip installation if databases are already installed and healthy"
          echo "Note: This may take 10-15 minutes for first installation"
          export NAMESPACE=${DB_NAMESPACE}
          export PG_DATABASE=postgres
          ./scripts/infrastructure/install-databases.sh
          echo "‚úÖ Shared databases check/install complete in ${DB_NAMESPACE} namespace"
          echo "‚úÖ PostgreSQL admin_user created (can create per-service databases)"
          echo "‚úÖ Each service will create its own database during deployment"
          echo "::endgroup::"
      
      - name: Install RabbitMQ (Shared Infrastructure)
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        timeout-minutes: 15
        env:
          # Priority: GitHub secrets required (no fallback to default)
          RABBITMQ_PASSWORD: ${{ secrets.RABBITMQ_PASSWORD || '' }}
          RABBITMQ_NAMESPACE: ${{ env.DB_NAMESPACE }}
          ENABLE_CLEANUP: ${{ env.ENABLE_CLEANUP }}
        run: |
          echo "::group::Installing RabbitMQ (Shared Infrastructure in ${RABBITMQ_NAMESPACE} namespace)"
          echo "Installing RabbitMQ as shared infrastructure..."
          echo "Namespace: ${RABBITMQ_NAMESPACE}"
          echo "This step will skip installation if RabbitMQ is already installed and healthy"
          echo "Note: This may take 5-10 minutes for first installation"
          ./scripts/infrastructure/install-rabbitmq.sh
          echo "‚úÖ RabbitMQ check/install complete in ${RABBITMQ_NAMESPACE} namespace"
          echo "‚úÖ All services can use shared RabbitMQ instance"
          echo "::endgroup::"

      - name: Install Argo CD (idempotent)
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        timeout-minutes: 15
        env:
          ARGOCD_DOMAIN: ${{ env.ARGOCD_DOMAIN }}
        run: |
          echo "::group::Installing Argo CD"
          echo "Installing ArgoCD GitOps tool (requires cert-manager for TLS)"
          echo "This step will skip if Argo CD is already installed and healthy"
          ./scripts/infrastructure/install-argocd.sh
          echo "‚úÖ ArgoCD check/install complete"
          echo "::endgroup::"

      - name: Bootstrap Argo CD applications (app-of-apps)
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        run: |
          echo "::group::Bootstrapping Argo CD applications"
          echo "This step uses kubectl apply (idempotent) - safe to rerun"
          # Apply the root Application that manages apps under apps/
          if [ -f apps/root-app.yaml ]; then
            kubectl apply -f apps/root-app.yaml
            echo "‚úÖ root Application applied/updated"
          else
            echo "‚ö†Ô∏è apps/root-app.yaml not found; skipping"
          fi
          
          # Also apply individual applications directly to ensure they're created
          echo "Applying individual ArgoCD Applications (idempotent)..."
          for app_file in apps/*/app.yaml; do
            if [ -f "$app_file" ]; then
              kubectl apply -f "$app_file" || echo "‚ö†Ô∏è Failed to apply $app_file"
              echo "‚úì Applied/updated: $app_file"
            fi
          done
          
          echo "Current ArgoCD Applications:"
          kubectl get applications -n argocd || true
          echo "::endgroup::"

      - name: Install Vertical Pod Autoscaler (VPA) [Optional]
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        timeout-minutes: 10
        continue-on-error: true
        run: |
          echo "::group::Installing Vertical Pod Autoscaler (Optional)"
          echo "Installing VPA for automatic pod resource optimization"
          echo "This step is optional and will skip if VPA is already installed"
          echo "Set FORCE_INSTALL=true to force reinstallation"
          ./scripts/infrastructure/install-vpa.sh || echo "‚ö†Ô∏è VPA installation skipped or failed (non-critical)"
          echo "‚úÖ VPA check/install complete"
          echo "::endgroup::"

      - name: Setup Git Access (SSH Keys & Configuration) [Optional]
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        continue-on-error: true
        run: |
          echo "::group::Setting up Git SSH Access (Optional)"
          echo "‚ö†Ô∏è  Note: Git SSH access setup is optional and requires manual GitHub deploy key configuration"
          echo "This step generates a key pair, but you must manually add the public key to GitHub"
          echo "This step will skip if SSH keys already exist"
          echo ""
          
          # Create SSH directory if it doesn't exist
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          
          # Generate SSH key pair for git operations (if not exists)
          if [ ! -f ~/.ssh/git_deploy_key ]; then
            echo "üîë Generating SSH key pair for git operations..."
            ssh-keygen -t ed25519 -C "vps-git-access@bengoerp" -f ~/.ssh/git_deploy_key -N ""
            chmod 600 ~/.ssh/git_deploy_key
            chmod 644 ~/.ssh/git_deploy_key.pub
          else
            echo "‚úÖ SSH key pair already exists"
          fi
          
          # Add GitHub to known hosts
          ssh-keyscan github.com >> ~/.ssh/known_hosts 2>/dev/null || true
          
          # Create SSH config for reliable git operations
          cat > ~/.ssh/config << 'EOF'
          Host github.com
              HostName github.com
              User git
              IdentityFile ~/.ssh/git_deploy_key
              IdentitiesOnly yes
              StrictHostKeyChecking no
          EOF
          chmod 600 ~/.ssh/config
          
          # Display public key for deploy key setup
          echo ""
          echo "üîë PUBLIC KEY FOR GITHUB DEPLOY KEY (MANUAL SETUP REQUIRED):"
          echo "============================================================="
          cat ~/.ssh/git_deploy_key.pub
          echo "============================================================="
          echo ""
          echo "üìã MANUAL STEPS:"
          echo "1. Copy the public key above"
          echo "2. Go to: https://github.com/Bengo-Hub/devops-k8s/settings/keys"
          echo "3. Add deploy key with 'Allow write access' enabled"
          echo "4. Test: git clone git@github.com:Bengo-Hub/devops-k8s.git"
          echo ""
          
          echo "::endgroup::"
      
      - name: Summary
        if: steps.kubeconfig_setup.outputs.kubeconfig_ready == 'true'
        env:
          ARGOCD_DOMAIN: ${{ env.ARGOCD_DOMAIN }}
          GRAFANA_DOMAIN: ${{ env.GRAFANA_DOMAIN }}
          DB_NAMESPACE: ${{ env.DB_NAMESPACE }}
        run: |
          echo "::group::Cluster Infrastructure Provisioning Complete"
          echo ""
          echo "‚úÖ Storage provisioner installed"
          echo "‚úÖ Shared Databases installed (PostgreSQL & Redis in ${DB_NAMESPACE} namespace)"
          echo "  - PostgreSQL admin_user created for managing per-service databases"
          echo "  - Each service creates its own database during deployment"
          echo "‚úÖ RabbitMQ installed (shared infrastructure in ${DB_NAMESPACE} namespace)"
          echo "‚úÖ NGINX Ingress Controller configured"
          echo "‚úÖ cert-manager installed"
          echo "‚úÖ Argo CD installed (https://${ARGOCD_DOMAIN})"
          echo "‚úÖ Monitoring Stack installed (https://${GRAFANA_DOMAIN}) in ${DB_NAMESPACE} namespace"
          echo "‚úÖ Vertical Pod Autoscaler (VPA) installed"
          echo ""
          echo "Shared Infrastructure Credentials (${DB_NAMESPACE} namespace):"
          echo "- PostgreSQL admin_user: kubectl get secret postgresql -n ${DB_NAMESPACE} -o jsonpath='{.data.admin-user-password}' | base64 -d"
          echo "- PostgreSQL postgres user: kubectl get secret postgresql -n ${DB_NAMESPACE} -o jsonpath='{.data.postgres-password}' | base64 -d"
          echo "- Redis: kubectl get secret redis -n ${DB_NAMESPACE} -o jsonpath='{.data.redis-password}' | base64 -d"
          echo "- RabbitMQ: kubectl get secret rabbitmq -n ${DB_NAMESPACE} -o jsonpath='{.data.rabbitmq-password}' | base64 -d"
          echo ""
          echo "Per-Service Databases:"
          echo "- Each service (cafe-backend, erp-api, treasury-app, notifications-app) creates its own database"
          echo "- Database names: cafe, bengo_erp, treasury, notifications"
          echo "- Databases are created automatically during service deployment"
          echo ""
          echo "Configuration (via GitHub Secrets - priority order):"
          echo "- ARGOCD_DOMAIN: ${ARGOCD_DOMAIN} (from secrets.ARGOCD_DOMAIN or default)"
          echo "- GRAFANA_DOMAIN: ${GRAFANA_DOMAIN} (from secrets.GRAFANA_DOMAIN or default)"
          echo "- DB_NAMESPACE: ${DB_NAMESPACE} (from secrets.DB_NAMESPACE or default)"
          echo ""
          echo "Next steps:"
          echo "1. Set up GitHub deploy key (see Git SSH Access step above - manual setup required)"
          echo "2. Point DNS to your VPS IP"
          echo "3. Configure Argo CD repository access"
          echo "4. Deploy applications via Argo CD (they will auto-create databases)"
          echo "5. Verify databases: kubectl -n ${DB_NAMESPACE} exec -it postgresql-0 -- psql -U admin_user -d postgres -c '\l'"
          echo "6. Verify VPA: kubectl get vpa --all-namespaces"
          echo ""
          echo "üìö Documentation:"
          echo "- Manual cluster setup: docs/contabo-setup-kubeadm.md"
          echo "- End-to-end workflow: docs/CLUSTER-SETUP-WORKFLOW.md"
          echo "- GitHub secrets: docs/github-secrets.md"
          echo "- Provisioning guide: docs/provisioning.md"
          echo "::endgroup::"
